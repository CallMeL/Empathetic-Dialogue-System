{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_syj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading GPT-2 encodings...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from chat import init_model as init_nanoGPT\n",
    "from chat import respond as get_respond_nanoGPT\n",
    "import torch\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data/emotion/validation/100_validation.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \n",
       "0                       Should have grabbed the gun.  \n",
       "1  Thank you!  It was so nice, I had no idea it w...  \n",
       "2        I do!  I'm feeling very optimistic about it  \n",
       "3  It was a medium amount of money but still, he ...  \n",
       "4  Well, I started walking much faster. It looked...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/Rope/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihyunryu/Project-ML/src/models/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/Relative/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihyunryu/Project-ML/src/models/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'rope':'Rope/withoutemotion/singleConversation',\n",
    "    'relative:': 'Relative/withoutemotion/singleConversation'\n",
    "}\n",
    "\n",
    "for model_type, model_path in model_list.items():\n",
    "    model_list[model_type] = init_nanoGPT(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_nanoGPT(row,model):\n",
    "    situation = row['Situation']\n",
    "    emotion = row['grouped_emotion']\n",
    "    human = row['empathetic_dialogues']\n",
    "    start = '<bot> ' + human + '<human>'\n",
    "    response, new_emotion, new_context = get_respond_nanoGPT(start, 1, model=model, enable_print=False)\n",
    "    return response #, new_emotion, new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    df[label] = df.apply(lambda row: get_response_from_nanoGPT(row, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./evaluation_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_rope</th>\n",
       "      <th>new_label_relative:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "      <td>What did you do</td>\n",
       "      <td>I'm sure you will do great!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "      <td>i am looking forward to it</td>\n",
       "      <td>I don't know. I have a good job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "      <td>I did, I did not think I would be able to go.</td>\n",
       "      <td>anywayap driverkeyes of tips to youlely waiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "      <td>I was so mad at him.</td>\n",
       "      <td>it alot I. I wasecake that guy at age of stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "      <td>I had to leave my house at the grocery store....</td>\n",
       "      <td>problems there you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       Should have grabbed the gun.   \n",
       "1  Thank you!  It was so nice, I had no idea it w...   \n",
       "2        I do!  I'm feeling very optimistic about it   \n",
       "3  It was a medium amount of money but still, he ...   \n",
       "4  Well, I started walking much faster. It looked...   \n",
       "\n",
       "                                      new_label_rope  \\\n",
       "0                                   What did you do    \n",
       "1                       i am looking forward to it     \n",
       "2     I did, I did not think I would be able to go.    \n",
       "3                              I was so mad at him.    \n",
       "4   I had to leave my house at the grocery store....   \n",
       "\n",
       "                                 new_label_relative:  \n",
       "0                       I'm sure you will do great!   \n",
       "1                  I don't know. I have a good job.   \n",
       "2   anywayap driverkeyes of tips to youlely waiti...  \n",
       "3   it alot I. I wasecake that guy at age of stud...  \n",
       "4                                problems there you   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(compared_column):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # To avoid 0 scores due to short sentences\n",
    "    for _, row in df.iterrows():\n",
    "        for ref, output in zip(row['labels'], row[compared_column]):\n",
    "            # Tokenize each sentence (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(reference_tokens, output_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = {}\n",
    "bleu_scores_average = {}\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # print(label)\n",
    "    bleu_scores[model_type] = get_bleu(label)\n",
    "    bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rope': 0.005843883992794925, 'relative:': 0.0057815062098800695}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(compared_column):\n",
    "    # Check for MPS device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "    # Calculate precision, recall, and F1 for each pair of reference and output\n",
    "    P, R, F1 = score(model_outputs, reference_sentences, lang='en', verbose=True, device = device)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.63 seconds, 17.77 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.67 seconds, 59.73 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    bert_scores[model_type] = {}\n",
    "    \n",
    "    # Calculate BERT score and assign it to the dictionary\n",
    "    bert_scores[model_type]['P'], bert_scores[model_type]['R'], bert_scores[model_type]['F1'] = calculate_bert_score(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: rope\n",
      "Average Precision: 0.85479736328125\n",
      "Average Recall: 0.8578989505767822\n",
      "Average F1: 0.8560744524002075\n",
      "--------------------------------------------------\n",
      "Model: relative:\n",
      "Average Precision: 0.8324218988418579\n",
      "Average Recall: 0.8207113742828369\n",
      "Average F1: 0.8262473344802856\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # P avarage\n",
    "    P_average = sum(bert_scores[model_type]['P']) / len(bert_scores[model_type]['P'])\n",
    "    # R avarage\n",
    "    R_average = sum(bert_scores[model_type]['R']) / len(bert_scores[model_type]['R'])\n",
    "    # F1 avarage\n",
    "    F1_average = sum(bert_scores[model_type]['F1']) / len(bert_scores[model_type]['F1'])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Precision: {P_average}\")\n",
    "    print(f\"Average Recall: {R_average}\")\n",
    "    print(f\"Average F1: {F1_average}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # GLUE - Sentiment Analysis Evaluation (SST-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def evaluate_sentiment(compared_column):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load multi-class sentiment or emotion pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        device=0 if device == \"mps\" else -1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "\n",
    "    for i, (output, reference) in enumerate(zip(model_outputs, reference_sentences), start=1):\n",
    "        output_sentiment = sentiment_pipeline(output)[0]['label']\n",
    "        reference_sentiment = sentiment_pipeline(reference)[0]['label']\n",
    "        \n",
    "        score = 1 if output_sentiment == reference_sentiment else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    sentiment_scores[model_type] = {}\n",
    "    \n",
    "    sentiment_scores[model_type] = evaluate_sentiment(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: rope\n",
      "Average Sentiment Score: 0.5\n",
      "--------------------------------------------------\n",
      "Model: relative:\n",
      "Average Sentiment Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    GLUE_average = sum(sentiment_scores[model_type]) / len(sentiment_scores[model_type])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Sentiment Score: {GLUE_average}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_syj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
