{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nanoGPT.chat import init_model as init_nanoGPT\n",
    "from  nanoGPT.chat import respond as get_respond_nanoGPT\n",
    "import torch\n",
    "from bert_score import score\n",
    "import tiktoken\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Validation dataset - Withouth emotion tag\n",
    "**(Facebook data + Chat GPT generated data)**\n",
    "- 6k pais of data\n",
    "- file already saved with model's outputs in data/emotion/validation/final.csv because takes a lot of time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/wholeConversation/ckpt.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiagermer/Desktop/SOFIA/IAS/WinterSemester_24_25/ml_proj/Project-ML/src/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withemotion/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withcontext/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation_withGPTdata/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=256/singleConversation_withGPTdata/ckpt.pt\n",
      "number of parameters: 3.45M\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'single_conversation': 'block_size=64/withoutemotion/singleConversation',\n",
    "    'whole_conversation':'block_size=64/withoutemotion/wholeConversation',\n",
    "    'single_conversation_withemotion':'block_size=64/withemotion',\n",
    "    'single_conversation_withcontext': 'block_size=64/withcontext',\n",
    "    'single_conversation_withGPTdata': 'block_size=64/withoutemotion/singleConversation_withGPTdata',\n",
    "    'single_conversation_withGPTdata_bs256': 'block_size=256/singleConversation_withGPTdata',\n",
    "}\n",
    "\n",
    "for model_type, model_path in model_list.items():\n",
    "    model_list[model_type] = init_nanoGPT(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For big validation dataset - without emotion tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing model's output, because we get (response, emotion, context), and we only want the response.\n",
    "\n",
    "test_file_path = '../../data/emotion/validation/final.csv'  \n",
    "test_df = pd.read_csv(test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_single_conversation</th>\n",
       "      <th>new_label_whole_conversation</th>\n",
       "      <th>new_label_single_conversation_withemotion</th>\n",
       "      <th>new_label_single_conversation_withcontext</th>\n",
       "      <th>new_label_single_conversation_withGPTdata</th>\n",
       "      <th>new_label_single_conversation_withGPTdata_bs256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it felt good to get approved for a vacation</td>\n",
       "      <td>That's great! Where ya headed?</td>\n",
       "      <td>That's great! You are prepared and prepared.</td>\n",
       "      <td>Yeah, I think I was a good job, I didn't get ...</td>\n",
       "      <td>oh no. I hope it turns out well for you!</td>\n",
       "      <td>That's awesome!  I hope you do well!</td>\n",
       "      <td>That sounds great! A fun experience are alway...</td>\n",
       "      <td>That's great to hear! Where are you planning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to disneyworld!</td>\n",
       "      <td>That sounds like a lot of fun! Who ya going with?</td>\n",
       "      <td>I am glad you are very excited! I'm sure you ...</td>\n",
       "      <td>I am! I never win anything.</td>\n",
       "      <td>I love to get a lot of fun, but I'm happy for...</td>\n",
       "      <td>I bet you are so excited!</td>\n",
       "      <td>That sounds exciting! What do you enjoy most ...</td>\n",
       "      <td>That sounds exciting! Enjoy theney films!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's great! Where ya headed?</td>\n",
       "      <td>to disneyworld!</td>\n",
       "      <td>I've never seen that song but I've never watc...</td>\n",
       "      <td>We are going to the beach.</td>\n",
       "      <td>I am going to Las Vegas for my birthday.</td>\n",
       "      <td>I have a big meeting with my girlfriend, I ha...</td>\n",
       "      <td>I'm here to assist you with any questions or ...</td>\n",
       "      <td>I'm here to assist you! What do you need help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I applied for an internship with a law office</td>\n",
       "      <td>Very nice! I bet your more than excited about it!</td>\n",
       "      <td>That's great! I bet your worked hard for it!</td>\n",
       "      <td>I hope so.  I am hoping to get a raise.</td>\n",
       "      <td>That's great!  Did you get the money back?</td>\n",
       "      <td>That's awesome! I hope you guys get it!</td>\n",
       "      <td>That is awesome.  What kind of job?</td>\n",
       "      <td>That's awesome. What position do you get?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am actually terrified. They probably laughed...</td>\n",
       "      <td>Oh no! Dont be anxious or worried! I'm sure yo...</td>\n",
       "      <td>I would be so nervous about it.</td>\n",
       "      <td>I am sure it will be worth it. I'm sure they ...</td>\n",
       "      <td>I know what you mean, that's a good idea.</td>\n",
       "      <td>I bet you are so envious!</td>\n",
       "      <td>It's understandable to feel scared. Stay calm...</td>\n",
       "      <td>It's understandable to feel scared. Stay safe!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                empathetic_dialogues  \\\n",
       "0        it felt good to get approved for a vacation   \n",
       "1                                    to disneyworld!   \n",
       "2                     That's great! Where ya headed?   \n",
       "3      I applied for an internship with a law office   \n",
       "4  I am actually terrified. They probably laughed...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                     That's great! Where ya headed?   \n",
       "1  That sounds like a lot of fun! Who ya going with?   \n",
       "2                                    to disneyworld!   \n",
       "3  Very nice! I bet your more than excited about it!   \n",
       "4  Oh no! Dont be anxious or worried! I'm sure yo...   \n",
       "\n",
       "                       new_label_single_conversation  \\\n",
       "0      That's great! You are prepared and prepared.    \n",
       "1   I am glad you are very excited! I'm sure you ...   \n",
       "2   I've never seen that song but I've never watc...   \n",
       "3      That's great! I bet your worked hard for it!    \n",
       "4                   I would be so nervous about it.    \n",
       "\n",
       "                        new_label_whole_conversation  \\\n",
       "0   Yeah, I think I was a good job, I didn't get ...   \n",
       "1                       I am! I never win anything.    \n",
       "2                        We are going to the beach.    \n",
       "3           I hope so.  I am hoping to get a raise.    \n",
       "4   I am sure it will be worth it. I'm sure they ...   \n",
       "\n",
       "           new_label_single_conversation_withemotion  \\\n",
       "0          oh no. I hope it turns out well for you!    \n",
       "1   I love to get a lot of fun, but I'm happy for...   \n",
       "2          I am going to Las Vegas for my birthday.    \n",
       "3        That's great!  Did you get the money back?    \n",
       "4         I know what you mean, that's a good idea.    \n",
       "\n",
       "           new_label_single_conversation_withcontext  \\\n",
       "0              That's awesome!  I hope you do well!    \n",
       "1                         I bet you are so excited!    \n",
       "2   I have a big meeting with my girlfriend, I ha...   \n",
       "3           That's awesome! I hope you guys get it!    \n",
       "4                         I bet you are so envious!    \n",
       "\n",
       "           new_label_single_conversation_withGPTdata  \\\n",
       "0   That sounds great! A fun experience are alway...   \n",
       "1   That sounds exciting! What do you enjoy most ...   \n",
       "2   I'm here to assist you with any questions or ...   \n",
       "3               That is awesome.  What kind of job?    \n",
       "4   It's understandable to feel scared. Stay calm...   \n",
       "\n",
       "     new_label_single_conversation_withGPTdata_bs256  \n",
       "0     That's great to hear! Where are you planning?   \n",
       "1         That sounds exciting! Enjoy theney films!   \n",
       "2   I'm here to assist you! What do you need help...  \n",
       "3         That's awesome. What position do you get?   \n",
       "4    It's understandable to feel scared. Stay safe!   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVG(BLEU) and BLEU-1,-2,-3,-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(AVG)BLEU with small test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(data, compared_column):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # To avoid 0 scores due to short sentences\n",
    "    for _, row in data.iterrows():\n",
    "        for ref, output in zip(row['labels'], row[compared_column]):\n",
    "            # Tokenize each sentence (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(reference_tokens, output_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_small_dataset(data):\n",
    "    bleu_scores = {}\n",
    "    bleu_scores_average = {}\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        # print(label)\n",
    "        bleu_scores[model_type] = get_bleu(data,label)\n",
    "        bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])\n",
    "    \n",
    "    return bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withoutemotion_single': 0.006294341508914749,\n",
       " 'withoutemotion_whole': 0.005549381274013914,\n",
       " 'withemotion': 0.005371129646648176,\n",
       " 'withcontext': 0.0056097142272521225,\n",
       " 'gpt_withoutemotion': 0.006452090556086916,\n",
       " 'gpt_blocksize_256': 0.00661592114219667}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_bleu_score_avg = bleu_small_dataset(df)\n",
    "\n",
    "small_bleu_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "gpt_blocksize_256: with a BLEU score of 0.006616.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = max(small_bleu_score_avg, key=small_bleu_score_avg.get)\n",
    "best_score = small_bleu_score_avg[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a BLEU score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bleu_score_avg = bleu_small_dataset(test_df)\n",
    "\n",
    "big_bleu_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best-performing model\n",
    "best_model = max(big_bleu_score_avg, key=big_bleu_score_avg.get)\n",
    "best_score = big_bleu_score_avg[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a BLEU score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVG(BLEU) with big test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(data, compared_column, weights):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # Avoid 0 scores due to short sentences\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # Extract reference and candidate sentences\n",
    "        reference_sentences = row['labels']\n",
    "        candidate_sentence = row[compared_column]\n",
    "\n",
    "        for ref, output in zip(reference_sentences, candidate_sentence):\n",
    "            # Tokenize sentences (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(\n",
    "                reference_tokens,\n",
    "                output_tokens,\n",
    "                weights=weights,\n",
    "                smoothing_function=smoothing_function\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_calculation(data, weights):\n",
    "    bleu_scores = {}\n",
    "    bleu_scores_average = {}\n",
    "\n",
    "    for model_type, _ in model_list.items():\n",
    "        # Dynamically construct the label column name\n",
    "        label = 'new_label_' + model_type\n",
    "\n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if label not in data.columns:\n",
    "            raise KeyError(f\"Column '{label}' does not exist in the DataFrame!\")\n",
    "\n",
    "        # Calculate BLEU scores for this model type\n",
    "        bleu_scores[model_type] = get_bleu(data, label, weights)\n",
    "        bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])\n",
    "\n",
    "    return bleu_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.03303577372298825,\n",
       " 'whole_conversation': 0.031149590503226302,\n",
       " 'single_conversation_withemotion': 0.03287725085675706,\n",
       " 'single_conversation_withcontext': 0.03263109583247091,\n",
       " 'single_conversation_withGPTdata': 0.036170623502072514,\n",
       " 'single_conversation_withGPTdata_bs256': 0.034916404327927836}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (1,0,0,0)\n",
    "\n",
    "bleu_1 = bleu_calculation(test_df, weights)\n",
    "\n",
    "bleu_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.010446828923058333,\n",
       " 'whole_conversation': 0.009850365417174566,\n",
       " 'single_conversation_withemotion': 0.010396699591207457,\n",
       " 'single_conversation_withcontext': 0.010318858537783629,\n",
       " 'single_conversation_withGPTdata': 0.011438155465496527,\n",
       " 'single_conversation_withGPTdata_bs256': 0.011041536537961272}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "bleu_2 = bleu_calculation(test_df,weights)\n",
    "\n",
    "bleu_2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.007117341692084967,\n",
       " 'whole_conversation': 0.006710975836043849,\n",
       " 'single_conversation_withemotion': 0.007083188975867789,\n",
       " 'single_conversation_withcontext': 0.007030156483523018,\n",
       " 'single_conversation_withGPTdata': 0.007792724603294758,\n",
       " 'single_conversation_withGPTdata_bs256': 0.007522511273526714}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (1/3, 1/3, 1/3, 0)\n",
    "\n",
    "bleu_3 = bleu_calculation(test_df, weights)\n",
    "\n",
    "bleu_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.00587468362062949,\n",
       " 'whole_conversation': 0.005539267542303131,\n",
       " 'single_conversation_withemotion': 0.005846493825725561,\n",
       " 'single_conversation_withcontext': 0.005802720584588993,\n",
       " 'single_conversation_withGPTdata': 0.006432147502200551,\n",
       " 'single_conversation_withGPTdata_bs256': 0.0062091122888948}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (1/4, 1/4, 1/4, 1/4)\n",
    "\n",
    "bleu_4 = bleu_calculation(test_df, weights)\n",
    "\n",
    "bleu_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(data,compared_column):\n",
    "    # Check for MPS device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_outputs = data['labels']\n",
    "    reference_sentences = data[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "    # Calculate precision, recall, and F1 for each pair of reference and output\n",
    "    P, R, F1 = score(model_outputs, reference_sentences, lang='en', verbose=True, device = device)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score_all_models(data):\n",
    "    bert_scores = {}\n",
    "\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        bert_scores[model_type] = {}\n",
    "        \n",
    "        # Calculate BERT score and assign it to the dictionary\n",
    "        bert_scores[model_type]['P'], bert_scores[model_type]['R'], bert_scores[model_type]['F1'] = calculate_bert_score(data, label)\n",
    "\n",
    "    return bert_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bert_scores(bert_scores):\n",
    "\n",
    "    avg_f1_scores ={}\n",
    "\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        # P avarage\n",
    "        P_average = sum(bert_scores[model_type]['P']) / len(bert_scores[model_type]['P'])\n",
    "        # R avarage\n",
    "        R_average = sum(bert_scores[model_type]['R']) / len(bert_scores[model_type]['R'])\n",
    "        # F1 avarage\n",
    "        F1_average = sum(bert_scores[model_type]['F1']) / len(bert_scores[model_type]['F1'])\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Model: {model_type}\")\n",
    "        print(f\"Average Precision: {P_average}\")\n",
    "        print(f\"Average Recall: {R_average}\")\n",
    "        print(f\"Average F1: {F1_average}\")\n",
    "\n",
    "        avg_f1_scores[model_type] = F1_average\n",
    "\n",
    "    return avg_f1_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Score for Big Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cd47c1255d4574b02159db61c012ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309139178ba949fca430f20db5066728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 135.93 seconds, 88.04 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c555b95aa944c6383caee945333d531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7b7d19d99490daeb4314be7ddeba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 181.02 seconds, 66.11 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a642713224323ad8219843ce109f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaaa263674147cf97ef68365a59a687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 180.65 seconds, 66.24 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bddea0a78946febdc103df07625ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a381bca29b47ce8c38987385423fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 129.50 seconds, 92.41 sentences/sec\n",
      "Using device: mps\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228a788f7fbf4ea6a87f348b3d27b7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facd50b78abf4217b344d25e6cf91cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 120.92 seconds, 98.96 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b027513d94849c5acec9c8f912f43fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb6f6c14d245bd88e3287747733096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 105.99 seconds, 112.91 sentences/sec\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Precision: 0.8516004681587219\n",
      "Average Recall: 0.8623329401016235\n",
      "Average F1: 0.8568010926246643\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Precision: 0.8422025442123413\n",
      "Average Recall: 0.8542978167533875\n",
      "Average F1: 0.8480852246284485\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Precision: 0.8510995507240295\n",
      "Average Recall: 0.8639686107635498\n",
      "Average F1: 0.8573451042175293\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Precision: 0.8292639851570129\n",
      "Average Recall: 0.8429385423660278\n",
      "Average F1: 0.8359043002128601\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Precision: 0.8533097505569458\n",
      "Average Recall: 0.8622663617134094\n",
      "Average F1: 0.8576485514640808\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Precision: 0.48458969593048096\n",
      "Average Recall: 0.4885256290435791\n",
      "Average F1: 0.48648038506507874\n"
     ]
    }
   ],
   "source": [
    "big_bert_scores = bert_score_all_models(data=test_df)\n",
    "\n",
    "big_bert_f1 = print_bert_scores(big_bert_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: gpt_withoutemotion\n",
      "Highest Average F1 Score: 0.8576\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model by highest F1 average\n",
    "best_model = max(big_bert_f1, key=big_bert_f1.get)\n",
    "best_f1_score = big_bert_f1[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average F1 Score: {best_f1_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE - Sentiment Analysis Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL USED:** \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
    "\n",
    "Supported Emotions: Joy, Anger, Sadness, Fear, Surprise, Love, Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(data, compared_column):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load multi-class sentiment or emotion pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        device=0 if device == \"mps\" else -1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    model_outputs = data['labels']\n",
    "    reference_sentences = data[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    \n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "\n",
    "    for i, (output, reference) in enumerate(zip(model_outputs, reference_sentences), start=1):\n",
    "        output_sentiment = sentiment_pipeline(output)[0]['label']\n",
    "        reference_sentiment = sentiment_pipeline(reference)[0]['label']\n",
    "        \n",
    "        score = 1 if output_sentiment == reference_sentiment else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_all_models(data):\n",
    "    sentiment_scores = {}\n",
    "\n",
    "    for model_type, _ in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        sentiment_scores[model_type] = {}\n",
    "        \n",
    "        sentiment_scores[model_type] = evaluate_sentiment(data, label)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_glue_scores(data):\n",
    "    \n",
    "    sentiment_scores = glue_all_models(data)\n",
    "\n",
    "    avg_glue_scores = {}\n",
    "    \n",
    "    for model_type, _ in model_list.items():\n",
    "        GLUE_average = sum(sentiment_scores[model_type]) / len(sentiment_scores[model_type])\n",
    "        avg_glue_scores[model_type] = GLUE_average\n",
    "\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Model: {model_type}\")\n",
    "        print(f\"Average Sentiment Score: {GLUE_average}\")\n",
    "\n",
    "    return avg_glue_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLUE Score for big dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "model_type withoutemotion_single\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Sentiment Score: 0.4727166374195705\n",
      "model_type withoutemotion_whole\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Sentiment Score: 0.43603242249519514\n",
      "model_type withemotion\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Sentiment Score: 0.4721316954959472\n",
      "model_type withcontext\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Sentiment Score: 0.45884515751650373\n",
      "model_type gpt_withoutemotion\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Sentiment Score: 0.47171387983621627\n",
      "model_type gpt_blocksize_256\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Sentiment Score: 0.34519929806969163\n"
     ]
    }
   ],
   "source": [
    "big_glue_scores = print_glue_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: withoutemotion_single\n",
      "Highest Average GLUE Score: 0.4727\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model based on the highest GLUE score\n",
    "best_model = max(big_glue_scores, key=big_glue_scores.get)\n",
    "best_glue_score = big_glue_scores[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average GLUE Score: {best_glue_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "def get_token_probabilities(reference_text, output_text, model):\n",
    "    try:\n",
    "        # Tokenize the input\n",
    "        input_ids = tokenizer.encode(output_text)  # List of token IDs\n",
    "        input_ids = torch.tensor([input_ids], dtype=torch.long)  # Convert to PyTorch tensor\n",
    "\n",
    "        # Pass the tokenized input to the model\n",
    "        logits, _ = model(input_ids)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Handle length mismatch\n",
    "        max_length = min(len(input_ids[0]), probs.size(1))  # to deal when reference has difference size of the model output\n",
    "\n",
    "        # Extract probabilities for the predicted tokens\n",
    "        token_probs = []\n",
    "        for i, token_id in enumerate(input_ids[0][:max_length]):\n",
    "            token_probs.append(probs[0, i, token_id].item())\n",
    "\n",
    "        return token_probs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token probabilities: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_perplexity(token_probs):\n",
    "    \"\"\"\n",
    "    Calculate sentence perplexity based on token probabilities.\n",
    "    \"\"\"\n",
    "    if not token_probs:  # Handle empty token probabilities\n",
    "        return float('inf')\n",
    "\n",
    "    log_probs = np.log(token_probs)\n",
    "    avg_log_prob = np.mean(log_probs)\n",
    "    return np.exp(-avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(data, compared_column, model):\n",
    "    \"\"\"\n",
    "    Calculate sentence-level perplexity using token probabilities.\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        reference_text = row['labels']\n",
    "        output_text = row[compared_column]\n",
    "        \n",
    "        # Query model for token probabilities\n",
    "        token_probs = get_token_probabilities(reference_text, output_text, model)\n",
    "        \n",
    "        # Calculate sentence-level perplexity\n",
    "        sentence_perplexity = calculate_sentence_perplexity(token_probs)\n",
    "        \n",
    "        perplexities.append(sentence_perplexity)\n",
    "    \n",
    "    print(f\"Completed token-based perplexity calculations for column: {compared_column}\")\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_scores_average(data):\n",
    "    # Compute perplexities for each model\n",
    "    perplexity_scores = {}\n",
    "    perplexity_scores_average = {}\n",
    "    \n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        perplexity_scores[model_type] = get_perplexity(data,label, model)\n",
    "        perplexity_scores_average[model_type] = sum(perplexity_scores[model_type]) / len(perplexity_scores[model_type])\n",
    "\n",
    "        print(f\"Average Perplexity for {model_type}: {perplexity_scores_average[model_type]}\")\n",
    "\n",
    "    return perplexity_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity Score for big dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-based perplexity for column: new_label_withoutemotion_single\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_single\n",
      "Average Perplexity for withoutemotion_single: 84066.95248548205\n",
      "Calculating token-based perplexity for column: new_label_withoutemotion_whole\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_whole\n",
      "Average Perplexity for withoutemotion_whole: 29549.33700155131\n",
      "Calculating token-based perplexity for column: new_label_withemotion\n",
      "Completed token-based perplexity calculations for column: new_label_withemotion\n",
      "Average Perplexity for withemotion: 2008198.4398578384\n",
      "Calculating token-based perplexity for column: new_label_withcontext\n",
      "Completed token-based perplexity calculations for column: new_label_withcontext\n",
      "Average Perplexity for withcontext: 63035.03316265359\n",
      "Calculating token-based perplexity for column: new_label_gpt_withoutemotion\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_withoutemotion\n",
      "Average Perplexity for gpt_withoutemotion: 28233.374812001217\n",
      "Calculating token-based perplexity for column: new_label_gpt_blocksize_256\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_blocksize_256\n",
      "Average Perplexity for gpt_blocksize_256: 218114.86185134976\n"
     ]
    }
   ],
   "source": [
    "big_perplexity = perplexity_scores_average(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "gpt_withoutemotion: with a Perplexity Score of 28233.374812.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = min(big_perplexity, key=big_perplexity.get)\n",
    "best_score = big_perplexity[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a Perplexity Score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
