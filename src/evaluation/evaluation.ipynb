{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nanoGPT.chat import init_model as init_nanoGPT\n",
    "from  nanoGPT.chat import respond as get_respond_nanoGPT\n",
    "import torch\n",
    "from bert_score import score\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Validation dataset  - With emotion tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/emotion/validation/100_validation.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \n",
       "0                       Should have grabbed the gun.  \n",
       "1  Thank you!  It was so nice, I had no idea it w...  \n",
       "2        I do!  I'm feeling very optimistic about it  \n",
       "3  It was a medium amount of money but still, he ...  \n",
       "4  Well, I started walking much faster. It looked...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Validation dataset - Withouth emotion tag\n",
    "(Facebook data + Chat GPT generated data)\n",
    "\n",
    "-> already saved with model's outputs in data/emotion/validation/final.csv because takes a lot of time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/wholeConversation/ckpt.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiagermer/Desktop/SOFIA/IAS/WinterSemester_24_25/ml_proj/Project-ML/src/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withemotion/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withcontext/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation_withGPTdata/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=256/singleConversation_withGPTdata/ckpt.pt\n",
      "number of parameters: 3.45M\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'single_conversation': 'block_size=64/withoutemotion/singleConversation',\n",
    "    'whole_conversation':'block_size=64/withoutemotion/wholeConversation',\n",
    "    'single_conversation_withemotion':'block_size=64/withemotion',\n",
    "    'single_conversation_withcontext': 'block_size=64/withcontext',\n",
    "    'single_conversation_withGPTdata': 'block_size=64/withoutemotion/singleConversation_withGPTdata',\n",
    "    'single_conversation_withGPTdata_bs256': 'block_size=256/singleConversation_withGPTdata',\n",
    "}\n",
    "\n",
    "for model_type, model_path in model_list.items():\n",
    "    model_list[model_type] = init_nanoGPT(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For small validation dataset - with emotion tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_nanoGPT(row,model):\n",
    "    situation = row['Situation']\n",
    "    emotion = row['grouped_emotion']\n",
    "    human = row['empathetic_dialogues']\n",
    "    start = '<bot> ' + human + '<human>'\n",
    "    response, new_emotion, new_context = get_respond_nanoGPT(start, 1, model=model, enable_print=False)\n",
    "    return response #, new_emotion, new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    df[label] = df.apply(lambda row: get_response_from_nanoGPT(row, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./evaluation_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_withoutemotion_single</th>\n",
       "      <th>new_label_withoutemotion_whole</th>\n",
       "      <th>new_label_withemotion</th>\n",
       "      <th>new_label_withcontext</th>\n",
       "      <th>new_label_gpt_withoutemotion</th>\n",
       "      <th>new_label_gpt_blocksize_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "      <td>I don't know.  It is always annoying.</td>\n",
       "      <td>Thank you! I don't have a sleep tonight.</td>\n",
       "      <td>I was too scared to go to the park and I had ...</td>\n",
       "      <td>No, I didn't have to pay my car in the car. I...</td>\n",
       "      <td>That sounds unsettling! It's important to avo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "      <td>I think they are going to do it all. They are...</td>\n",
       "      <td>Thank you. I'm sure they were a good friend.</td>\n",
       "      <td>Yes, it was a great time.</td>\n",
       "      <td>I did. I was so happy to see that!</td>\n",
       "      <td>Thank you! I appreciate your support! How can...</td>\n",
       "      <td>That was really nice of them!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "      <td>I did, but I am not sure how I am going to do...</td>\n",
       "      <td>Yes, we did.  We were so happy.</td>\n",
       "      <td>I did. It was so good to see a lot of friends...</td>\n",
       "      <td>I did, but I was very happy and I was happy.</td>\n",
       "      <td>Yes, I'm not sure how to win!</td>\n",
       "      <td>I'm here to help! What do you need assistance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "      <td>That's a bummer.  Did you find your money?</td>\n",
       "      <td>It was a good thing, but it was really good t...</td>\n",
       "      <td>Not really but I didn't get it.</td>\n",
       "      <td>It was a very nice car. I was so angry about ...</td>\n",
       "      <td>No, I'm not sure but it's just a little bit s...</td>\n",
       "      <td>Yes, he was a great amount of money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "      <td>I was a police officer, and I was so mad at him</td>\n",
       "      <td>It was a really scary time. I had so much fun.</td>\n",
       "      <td>My dog was being a kid.</td>\n",
       "      <td>I had to go to the doctor. I was so mad at my...</td>\n",
       "      <td>I'm here to help! What’s on your mind?</td>\n",
       "      <td>It was a good way to work, but I can’t unders...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       Should have grabbed the gun.   \n",
       "1  Thank you!  It was so nice, I had no idea it w...   \n",
       "2        I do!  I'm feeling very optimistic about it   \n",
       "3  It was a medium amount of money but still, he ...   \n",
       "4  Well, I started walking much faster. It looked...   \n",
       "\n",
       "                     new_label_withoutemotion_single  \\\n",
       "0            I don't know.  It is always annoying.     \n",
       "1   I think they are going to do it all. They are...   \n",
       "2   I did, but I am not sure how I am going to do...   \n",
       "3        That's a bummer.  Did you find your money?    \n",
       "4   I was a police officer, and I was so mad at him    \n",
       "\n",
       "                      new_label_withoutemotion_whole  \\\n",
       "0          Thank you! I don't have a sleep tonight.    \n",
       "1      Thank you. I'm sure they were a good friend.    \n",
       "2                   Yes, we did.  We were so happy.    \n",
       "3   It was a good thing, but it was really good t...   \n",
       "4    It was a really scary time. I had so much fun.    \n",
       "\n",
       "                               new_label_withemotion  \\\n",
       "0   I was too scared to go to the park and I had ...   \n",
       "1                         Yes, it was a great time.    \n",
       "2   I did. It was so good to see a lot of friends...   \n",
       "3                   Not really but I didn't get it.    \n",
       "4                           My dog was being a kid.    \n",
       "\n",
       "                               new_label_withcontext  \\\n",
       "0   No, I didn't have to pay my car in the car. I...   \n",
       "1                I did. I was so happy to see that!    \n",
       "2      I did, but I was very happy and I was happy.    \n",
       "3   It was a very nice car. I was so angry about ...   \n",
       "4   I had to go to the doctor. I was so mad at my...   \n",
       "\n",
       "                        new_label_gpt_withoutemotion  \\\n",
       "0   That sounds unsettling! It's important to avo...   \n",
       "1   Thank you! I appreciate your support! How can...   \n",
       "2                     Yes, I'm not sure how to win!    \n",
       "3   No, I'm not sure but it's just a little bit s...   \n",
       "4            I'm here to help! What’s on your mind?    \n",
       "\n",
       "                         new_label_gpt_blocksize_256  \n",
       "0                                                     \n",
       "1                     That was really nice of them!   \n",
       "2   I'm here to help! What do you need assistance...  \n",
       "3              Yes, he was a great amount of money.   \n",
       "4   It was a good way to work, but I can’t unders...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For big validation dataset - without emotion tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing model's output, because we get (response, emotion, context), and we only want the response.\n",
    "\n",
    "test_file_path = '../../data/emotion/validation/final.csv'  \n",
    "test_df = pd.read_csv(test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_single_conversation</th>\n",
       "      <th>new_label_whole_conversation</th>\n",
       "      <th>new_label_single_conversation_withemotion</th>\n",
       "      <th>new_label_single_conversation_withcontext</th>\n",
       "      <th>new_label_single_conversation_withGPTdata</th>\n",
       "      <th>new_label_single_conversation_withGPTdata_bs256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it felt good to get approved for a vacation</td>\n",
       "      <td>That's great! Where ya headed?</td>\n",
       "      <td>That's great! You are prepared and prepared.</td>\n",
       "      <td>Yeah, I think I was a good job, I didn't get ...</td>\n",
       "      <td>oh no. I hope it turns out well for you!</td>\n",
       "      <td>That's awesome!  I hope you do well!</td>\n",
       "      <td>That sounds great! A fun experience are alway...</td>\n",
       "      <td>That's great to hear! Where are you planning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to disneyworld!</td>\n",
       "      <td>That sounds like a lot of fun! Who ya going with?</td>\n",
       "      <td>I am glad you are very excited! I'm sure you ...</td>\n",
       "      <td>I am! I never win anything.</td>\n",
       "      <td>I love to get a lot of fun, but I'm happy for...</td>\n",
       "      <td>I bet you are so excited!</td>\n",
       "      <td>That sounds exciting! What do you enjoy most ...</td>\n",
       "      <td>That sounds exciting! Enjoy theney films!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's great! Where ya headed?</td>\n",
       "      <td>to disneyworld!</td>\n",
       "      <td>I've never seen that song but I've never watc...</td>\n",
       "      <td>We are going to the beach.</td>\n",
       "      <td>I am going to Las Vegas for my birthday.</td>\n",
       "      <td>I have a big meeting with my girlfriend, I ha...</td>\n",
       "      <td>I'm here to assist you with any questions or ...</td>\n",
       "      <td>I'm here to assist you! What do you need help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I applied for an internship with a law office</td>\n",
       "      <td>Very nice! I bet your more than excited about it!</td>\n",
       "      <td>That's great! I bet your worked hard for it!</td>\n",
       "      <td>I hope so.  I am hoping to get a raise.</td>\n",
       "      <td>That's great!  Did you get the money back?</td>\n",
       "      <td>That's awesome! I hope you guys get it!</td>\n",
       "      <td>That is awesome.  What kind of job?</td>\n",
       "      <td>That's awesome. What position do you get?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am actually terrified. They probably laughed...</td>\n",
       "      <td>Oh no! Dont be anxious or worried! I'm sure yo...</td>\n",
       "      <td>I would be so nervous about it.</td>\n",
       "      <td>I am sure it will be worth it. I'm sure they ...</td>\n",
       "      <td>I know what you mean, that's a good idea.</td>\n",
       "      <td>I bet you are so envious!</td>\n",
       "      <td>It's understandable to feel scared. Stay calm...</td>\n",
       "      <td>It's understandable to feel scared. Stay safe!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                empathetic_dialogues  \\\n",
       "0        it felt good to get approved for a vacation   \n",
       "1                                    to disneyworld!   \n",
       "2                     That's great! Where ya headed?   \n",
       "3      I applied for an internship with a law office   \n",
       "4  I am actually terrified. They probably laughed...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                     That's great! Where ya headed?   \n",
       "1  That sounds like a lot of fun! Who ya going with?   \n",
       "2                                    to disneyworld!   \n",
       "3  Very nice! I bet your more than excited about it!   \n",
       "4  Oh no! Dont be anxious or worried! I'm sure yo...   \n",
       "\n",
       "                       new_label_single_conversation  \\\n",
       "0      That's great! You are prepared and prepared.    \n",
       "1   I am glad you are very excited! I'm sure you ...   \n",
       "2   I've never seen that song but I've never watc...   \n",
       "3      That's great! I bet your worked hard for it!    \n",
       "4                   I would be so nervous about it.    \n",
       "\n",
       "                        new_label_whole_conversation  \\\n",
       "0   Yeah, I think I was a good job, I didn't get ...   \n",
       "1                       I am! I never win anything.    \n",
       "2                        We are going to the beach.    \n",
       "3           I hope so.  I am hoping to get a raise.    \n",
       "4   I am sure it will be worth it. I'm sure they ...   \n",
       "\n",
       "           new_label_single_conversation_withemotion  \\\n",
       "0          oh no. I hope it turns out well for you!    \n",
       "1   I love to get a lot of fun, but I'm happy for...   \n",
       "2          I am going to Las Vegas for my birthday.    \n",
       "3        That's great!  Did you get the money back?    \n",
       "4         I know what you mean, that's a good idea.    \n",
       "\n",
       "           new_label_single_conversation_withcontext  \\\n",
       "0              That's awesome!  I hope you do well!    \n",
       "1                         I bet you are so excited!    \n",
       "2   I have a big meeting with my girlfriend, I ha...   \n",
       "3           That's awesome! I hope you guys get it!    \n",
       "4                         I bet you are so envious!    \n",
       "\n",
       "           new_label_single_conversation_withGPTdata  \\\n",
       "0   That sounds great! A fun experience are alway...   \n",
       "1   That sounds exciting! What do you enjoy most ...   \n",
       "2   I'm here to assist you with any questions or ...   \n",
       "3               That is awesome.  What kind of job?    \n",
       "4   It's understandable to feel scared. Stay calm...   \n",
       "\n",
       "     new_label_single_conversation_withGPTdata_bs256  \n",
       "0     That's great to hear! Where are you planning?   \n",
       "1         That sounds exciting! Enjoy theney films!   \n",
       "2   I'm here to assist you! What do you need help...  \n",
       "3         That's awesome. What position do you get?   \n",
       "4    It's understandable to feel scared. Stay safe!   "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU with small test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(data, compared_column):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # To avoid 0 scores due to short sentences\n",
    "    for _, row in data.iterrows():\n",
    "        for ref, output in zip(row['labels'], row[compared_column]):\n",
    "            # Tokenize each sentence (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(reference_tokens, output_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_small_dataset(data):\n",
    "    bleu_scores = {}\n",
    "    bleu_scores_average = {}\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        # print(label)\n",
    "        bleu_scores[model_type] = get_bleu(data,label)\n",
    "        bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])\n",
    "    \n",
    "    return bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withoutemotion_single': 0.006294341508914749,\n",
       " 'withoutemotion_whole': 0.005549381274013914,\n",
       " 'withemotion': 0.005371129646648176,\n",
       " 'withcontext': 0.0056097142272521225,\n",
       " 'gpt_withoutemotion': 0.006452090556086916,\n",
       " 'gpt_blocksize_256': 0.00661592114219667}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_bleu_score_avg = bleu_small_dataset(df)\n",
    "\n",
    "small_bleu_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "gpt_blocksize_256: with a BLEU score of 0.006616.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = max(small_bleu_score_avg, key=small_bleu_score_avg.get)\n",
    "best_score = small_bleu_score_avg[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a BLEU score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU with big test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_small_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m big_bleu_score_avg \u001b[38;5;241m=\u001b[39m \u001b[43mbleu_small_dataset\u001b[49m(test_df)\n\u001b[1;32m      3\u001b[0m big_bleu_score_avg\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_small_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "big_bleu_score_avg = bleu_small_dataset(test_df)\n",
    "\n",
    "big_bleu_score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "gpt_withoutemotion: with a BLEU score of 0.006432.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = max(big_bleu_score_avg, key=big_bleu_score_avg.get)\n",
    "best_score = big_bleu_score_avg[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a BLEU score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(data, compared_column, weights):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # Avoid 0 scores due to short sentences\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # Extract reference and candidate sentences\n",
    "        reference_sentences = row['labels']\n",
    "        candidate_sentence = row[compared_column]\n",
    "\n",
    "        for ref, output in zip(reference_sentences, candidate_sentence):\n",
    "            # Tokenize sentences (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(\n",
    "                reference_tokens,\n",
    "                output_tokens,\n",
    "                weights=weights,\n",
    "                smoothing_function=smoothing_function\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_calculation(data, weights):\n",
    "    bleu_scores = {}\n",
    "    bleu_scores_average = {}\n",
    "\n",
    "    for model_type, _ in model_list.items():\n",
    "        # Dynamically construct the label column name\n",
    "        label = 'new_label_' + model_type\n",
    "\n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if label not in data.columns:\n",
    "            raise KeyError(f\"Column '{label}' does not exist in the DataFrame!\")\n",
    "\n",
    "        # Calculate BLEU scores for this model type\n",
    "        bleu_scores[model_type] = get_bleu(data, label, weights)\n",
    "        bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])\n",
    "\n",
    "    return bleu_scores_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.010446828923058333,\n",
       " 'whole_conversation': 0.009850365417174566,\n",
       " 'single_conversation_withemotion': 0.010396699591207457,\n",
       " 'single_conversation_withcontext': 0.010318858537783629,\n",
       " 'single_conversation_withGPTdata': 0.011438155465496527,\n",
       " 'single_conversation_withGPTdata_bs256': 0.011041536537961272}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (0.5, 0.5, 0, 0)\n",
    "\n",
    "bleu_2 = bleu_calculation(test_df,weights)\n",
    "\n",
    "bleu_2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.007117341692084967,\n",
       " 'whole_conversation': 0.006710975836043849,\n",
       " 'single_conversation_withemotion': 0.007083188975867789,\n",
       " 'single_conversation_withcontext': 0.007030156483523018,\n",
       " 'single_conversation_withGPTdata': 0.007792724603294758,\n",
       " 'single_conversation_withGPTdata_bs256': 0.007522511273526714}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (1/3, 1/3, 1/3, 0)\n",
    "\n",
    "bleu_3 = bleu_calculation(test_df, weights)\n",
    "\n",
    "bleu_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single_conversation': 0.00587468362062949,\n",
       " 'whole_conversation': 0.005539267542303131,\n",
       " 'single_conversation_withemotion': 0.005846493825725561,\n",
       " 'single_conversation_withcontext': 0.005802720584588993,\n",
       " 'single_conversation_withGPTdata': 0.006432147502200551,\n",
       " 'single_conversation_withGPTdata_bs256': 0.0062091122888948}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = (1/4, 1/4, 1/4, 1/4)\n",
    "\n",
    "bleu_4 = bleu_calculation(test_df, weights)\n",
    "\n",
    "bleu_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(data,compared_column):\n",
    "    # Check for MPS device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_outputs = data['labels']\n",
    "    reference_sentences = data[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "    # Calculate precision, recall, and F1 for each pair of reference and output\n",
    "    P, R, F1 = score(model_outputs, reference_sentences, lang='en', verbose=True, device = device)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score_all_models(data):\n",
    "    bert_scores = {}\n",
    "\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        bert_scores[model_type] = {}\n",
    "        \n",
    "        # Calculate BERT score and assign it to the dictionary\n",
    "        bert_scores[model_type]['P'], bert_scores[model_type]['R'], bert_scores[model_type]['F1'] = calculate_bert_score(data, label)\n",
    "\n",
    "    return bert_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bert_scores(bert_scores):\n",
    "\n",
    "    avg_f1_scores ={}\n",
    "\n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        # P avarage\n",
    "        P_average = sum(bert_scores[model_type]['P']) / len(bert_scores[model_type]['P'])\n",
    "        # R avarage\n",
    "        R_average = sum(bert_scores[model_type]['R']) / len(bert_scores[model_type]['R'])\n",
    "        # F1 avarage\n",
    "        F1_average = sum(bert_scores[model_type]['F1']) / len(bert_scores[model_type]['F1'])\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Model: {model_type}\")\n",
    "        print(f\"Average Precision: {P_average}\")\n",
    "        print(f\"Average Recall: {R_average}\")\n",
    "        print(f\"Average F1: {F1_average}\")\n",
    "\n",
    "        avg_f1_scores[model_type] = F1_average\n",
    "\n",
    "    return avg_f1_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert Score for small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8777c0fe1e4227a8a8050f98f5203e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d5426aff214379b4aea6c2ac048917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.65 seconds, 60.44 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1319b7d9b5c43cdaad8389771565d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcf0745b4a54b1bbf61e881c89dbf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.56 seconds, 64.30 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0dc4e5c3034cd7ad5acc875300b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c3e4815564710a448ee4ee585ca37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.56 seconds, 64.12 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d28c6f4aaa43b4b785a0edcf18f197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b32028af2d9487cb91226df90ef1231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.61 seconds, 62.19 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472b445e90704cc5aa17b880a745e6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eadd5b4b493429fa8002b1789718efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.63 seconds, 61.32 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449c99fc712a4587902503b84442966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf60bdffc0c141ba920660f3058ed425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.43 seconds, 69.83 sentences/sec\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Precision: 0.8547204732894897\n",
      "Average Recall: 0.8605411052703857\n",
      "Average F1: 0.8573648929595947\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Precision: 0.779451310634613\n",
      "Average Recall: 0.7860279083251953\n",
      "Average F1: 0.7825521230697632\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Precision: 0.8557742834091187\n",
      "Average Recall: 0.8639095425605774\n",
      "Average F1: 0.8595598340034485\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Precision: 0.8496300578117371\n",
      "Average Recall: 0.8560236096382141\n",
      "Average F1: 0.8526116013526917\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Precision: 0.8567517995834351\n",
      "Average Recall: 0.8607215881347656\n",
      "Average F1: 0.8585449457168579\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Precision: 0.5550978183746338\n",
      "Average Recall: 0.5574827194213867\n",
      "Average F1: 0.5561996102333069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    }
   ],
   "source": [
    "small_bert_scores = bert_score_all_models(data=df)\n",
    "\n",
    "small_bert_f1 = print_bert_scores(small_bert_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: withemotion\n",
      "Highest Average F1 Score: 0.8596\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model by highest F1 average\n",
    "best_model = max(small_bert_f1, key=small_bert_f1.get)\n",
    "best_f1_score = small_bert_f1[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average F1 Score: {best_f1_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Score for Big Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cd47c1255d4574b02159db61c012ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309139178ba949fca430f20db5066728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 135.93 seconds, 88.04 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c555b95aa944c6383caee945333d531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7b7d19d99490daeb4314be7ddeba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 181.02 seconds, 66.11 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a642713224323ad8219843ce109f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaaa263674147cf97ef68365a59a687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 180.65 seconds, 66.24 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bddea0a78946febdc103df07625ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a381bca29b47ce8c38987385423fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 129.50 seconds, 92.41 sentences/sec\n",
      "Using device: mps\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228a788f7fbf4ea6a87f348b3d27b7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facd50b78abf4217b344d25e6cf91cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 120.92 seconds, 98.96 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b027513d94849c5acec9c8f912f43fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb6f6c14d245bd88e3287747733096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 105.99 seconds, 112.91 sentences/sec\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Precision: 0.8516004681587219\n",
      "Average Recall: 0.8623329401016235\n",
      "Average F1: 0.8568010926246643\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Precision: 0.8422025442123413\n",
      "Average Recall: 0.8542978167533875\n",
      "Average F1: 0.8480852246284485\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Precision: 0.8510995507240295\n",
      "Average Recall: 0.8639686107635498\n",
      "Average F1: 0.8573451042175293\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Precision: 0.8292639851570129\n",
      "Average Recall: 0.8429385423660278\n",
      "Average F1: 0.8359043002128601\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Precision: 0.8533097505569458\n",
      "Average Recall: 0.8622663617134094\n",
      "Average F1: 0.8576485514640808\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Precision: 0.48458969593048096\n",
      "Average Recall: 0.4885256290435791\n",
      "Average F1: 0.48648038506507874\n"
     ]
    }
   ],
   "source": [
    "big_bert_scores = bert_score_all_models(data=test_df)\n",
    "\n",
    "big_bert_f1 = print_bert_scores(big_bert_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: gpt_withoutemotion\n",
      "Highest Average F1 Score: 0.8576\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model by highest F1 average\n",
    "best_model = max(big_bert_f1, key=big_bert_f1.get)\n",
    "best_f1_score = big_bert_f1[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average F1 Score: {best_f1_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE - Sentiment Analysis Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def evaluate_sentiment(data, compared_column):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load multi-class sentiment or emotion pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        device=0 if device == \"mps\" else -1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    model_outputs = data['labels']\n",
    "    reference_sentences = data[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    \n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "\n",
    "    for i, (output, reference) in enumerate(zip(model_outputs, reference_sentences), start=1):\n",
    "        output_sentiment = sentiment_pipeline(output)[0]['label']\n",
    "        reference_sentiment = sentiment_pipeline(reference)[0]['label']\n",
    "        \n",
    "        score = 1 if output_sentiment == reference_sentiment else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_all_models(data):\n",
    "    sentiment_scores = {}\n",
    "\n",
    "    for model_type, _ in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        sentiment_scores[model_type] = {}\n",
    "        \n",
    "        sentiment_scores[model_type] = evaluate_sentiment(data, label)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_glue_scores(data):\n",
    "    \n",
    "    sentiment_scores = glue_all_models(data)\n",
    "\n",
    "    avg_glue_scores = {}\n",
    "    \n",
    "    for model_type, _ in model_list.items():\n",
    "        GLUE_average = sum(sentiment_scores[model_type]) / len(sentiment_scores[model_type])\n",
    "        avg_glue_scores[model_type] = GLUE_average\n",
    "\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Model: {model_type}\")\n",
    "        print(f\"Average Sentiment Score: {GLUE_average}\")\n",
    "\n",
    "    return avg_glue_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLUE Score for small dataset** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "model_type withoutemotion_single\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Sentiment Score: 0.51\n",
      "model_type withoutemotion_whole\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Sentiment Score: 0.4\n",
      "model_type withemotion\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Sentiment Score: 0.54\n",
      "model_type withcontext\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Sentiment Score: 0.42\n",
      "model_type gpt_withoutemotion\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Sentiment Score: 0.48\n",
      "model_type gpt_blocksize_256\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Sentiment Score: 0.35\n"
     ]
    }
   ],
   "source": [
    "small_glue_scores = print_glue_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: withemotion\n",
      "Highest Average GLUE Score: 0.5400\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model based on the highest GLUE score\n",
    "best_model = max(small_glue_scores, key=small_glue_scores.get)\n",
    "best_glue_score = small_glue_scores[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average GLUE Score: {best_glue_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLUE Score for big dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "model_type withoutemotion_single\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Sentiment Score: 0.4727166374195705\n",
      "model_type withoutemotion_whole\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Sentiment Score: 0.43603242249519514\n",
      "model_type withemotion\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Sentiment Score: 0.4721316954959472\n",
      "model_type withcontext\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Sentiment Score: 0.45884515751650373\n",
      "model_type gpt_withoutemotion\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Sentiment Score: 0.47171387983621627\n",
      "model_type gpt_blocksize_256\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Sentiment Score: 0.34519929806969163\n"
     ]
    }
   ],
   "source": [
    "big_glue_scores = print_glue_scores(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best Model: withoutemotion_single\n",
      "Highest Average GLUE Score: 0.4727\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model based on the highest GLUE score\n",
    "best_model = max(big_glue_scores, key=big_glue_scores.get)\n",
    "best_glue_score = big_glue_scores[best_model]\n",
    "\n",
    "# Print the best model\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Highest Average GLUE Score: {best_glue_score:.4f}\")\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "def get_token_probabilities(reference_text, output_text, model):\n",
    "    try:\n",
    "        # Tokenize the input\n",
    "        input_ids = tokenizer.encode(output_text)  # List of token IDs\n",
    "        input_ids = torch.tensor([input_ids], dtype=torch.long)  # Convert to PyTorch tensor\n",
    "\n",
    "        # Pass the tokenized input to the model\n",
    "        logits, _ = model(input_ids)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Handle length mismatch\n",
    "        max_length = min(len(input_ids[0]), probs.size(1))  # to deal when reference has difference size of the model output\n",
    "\n",
    "        # Extract probabilities for the predicted tokens\n",
    "        token_probs = []\n",
    "        for i, token_id in enumerate(input_ids[0][:max_length]):\n",
    "            token_probs.append(probs[0, i, token_id].item())\n",
    "\n",
    "        return token_probs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching token probabilities: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_perplexity(token_probs):\n",
    "    \"\"\"\n",
    "    Calculate sentence perplexity based on token probabilities.\n",
    "    \"\"\"\n",
    "    if not token_probs:  # Handle empty token probabilities\n",
    "        return float('inf')\n",
    "\n",
    "    log_probs = np.log(token_probs)\n",
    "    avg_log_prob = np.mean(log_probs)\n",
    "    return np.exp(-avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(data, compared_column, model):\n",
    "    \"\"\"\n",
    "    Calculate sentence-level perplexity using token probabilities.\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        reference_text = row['labels']\n",
    "        output_text = row[compared_column]\n",
    "        \n",
    "        # Query model for token probabilities\n",
    "        token_probs = get_token_probabilities(reference_text, output_text, model)\n",
    "        \n",
    "        # Calculate sentence-level perplexity\n",
    "        sentence_perplexity = calculate_sentence_perplexity(token_probs)\n",
    "        \n",
    "        perplexities.append(sentence_perplexity)\n",
    "    \n",
    "    print(f\"Completed token-based perplexity calculations for column: {compared_column}\")\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_scores_average(data):\n",
    "    # Compute perplexities for each model\n",
    "    perplexity_scores = {}\n",
    "    perplexity_scores_average = {}\n",
    "    \n",
    "    for model_type, model in model_list.items():\n",
    "        label = 'new_label_' + model_type\n",
    "        perplexity_scores[model_type] = get_perplexity(data,label, model)\n",
    "        perplexity_scores_average[model_type] = sum(perplexity_scores[model_type]) / len(perplexity_scores[model_type])\n",
    "\n",
    "        print(f\"Average Perplexity for {model_type}: {perplexity_scores_average[model_type]}\")\n",
    "\n",
    "    return perplexity_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity Score for small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-based perplexity for column: new_label_withoutemotion_single\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_single\n",
      "Average Perplexity for withoutemotion_single: 2864.188433546633\n",
      "Calculating token-based perplexity for column: new_label_withoutemotion_whole\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([sub_12, add_12, add_15, add_25, add_21, a...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([rsqrt_8, sub_12, add_9, add_24, add_15, e...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_18, add, add_6, add_25, add_21, embed...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_12, add_25, mul_36, add_21, rsqrt_8, ...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_9, sub_12, embedding, mul_37, add_12,...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_9, var_mean_8, add_18, add_15, add_24...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_12, mul_37, add_3, var_mean_8, add_6,...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([sub_12, mul_36, mul_37, embedding_1, add_...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error fetching token probabilities: backend='inductor' raised:\n",
      "LoweringException: IndexError: index is out of bounds for dimension with size 0\n",
      "  target: aten.index.Tensor\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.float32,\n",
      "      def inner_fn(index):\n",
      "          _, i1, i2 = index\n",
      "          tmp0 = ops.constant(nan, torch.float32)\n",
      "          tmp1 = ops.load(primals_28, i2)\n",
      "          tmp2 = tmp0 * tmp1\n",
      "          return tmp2\n",
      "      ,\n",
      "      ranges=[1, 0, 64],\n",
      "      origin_node=mul_37,\n",
      "      origins=OrderedSet([add_25, add_3, add_15, add_18, add_21, ad...\n",
      "    )\n",
      "  ))\n",
      "  args[1]: [None, TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      def inner_fn(index):\n",
      "          _ = index\n",
      "          tmp0 = ops.constant(-1, torch.int64)\n",
      "          return tmp0\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origin_node=full_default_16,\n",
      "      origins=OrderedSet([full_default_16])\n",
      "    )\n",
      "  ))]\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_whole\n",
      "Average Perplexity for withoutemotion_whole: inf\n",
      "Calculating token-based perplexity for column: new_label_withemotion\n",
      "Completed token-based perplexity calculations for column: new_label_withemotion\n",
      "Average Perplexity for withemotion: 937750.794195298\n",
      "Calculating token-based perplexity for column: new_label_withcontext\n",
      "Completed token-based perplexity calculations for column: new_label_withcontext\n",
      "Average Perplexity for withcontext: 2825.1275006949722\n",
      "Calculating token-based perplexity for column: new_label_gpt_withoutemotion\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_withoutemotion\n",
      "Average Perplexity for gpt_withoutemotion: 7074.3668454626595\n",
      "Calculating token-based perplexity for column: new_label_gpt_blocksize_256\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_blocksize_256\n",
      "Average Perplexity for gpt_blocksize_256: 24069.64634160262\n"
     ]
    }
   ],
   "source": [
    "small_perplexity = perplexity_scores_average(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withoutemotion_single': 2864.188433546633,\n",
       " 'withoutemotion_whole': inf,\n",
       " 'withemotion': 937750.794195298,\n",
       " 'withcontext': 2825.1275006949722,\n",
       " 'gpt_withoutemotion': 7074.3668454626595,\n",
       " 'gpt_blocksize_256': 24069.64634160262}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "withcontext: with a Perplexity Score of 2825.127501.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = min(small_perplexity, key=small_perplexity.get)\n",
    "best_score = small_perplexity[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a Perplexity Score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity Score for big dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-based perplexity for column: new_label_withoutemotion_single\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_single\n",
      "Average Perplexity for withoutemotion_single: 84066.95248548205\n",
      "Calculating token-based perplexity for column: new_label_withoutemotion_whole\n",
      "Completed token-based perplexity calculations for column: new_label_withoutemotion_whole\n",
      "Average Perplexity for withoutemotion_whole: 29549.33700155131\n",
      "Calculating token-based perplexity for column: new_label_withemotion\n",
      "Completed token-based perplexity calculations for column: new_label_withemotion\n",
      "Average Perplexity for withemotion: 2008198.4398578384\n",
      "Calculating token-based perplexity for column: new_label_withcontext\n",
      "Completed token-based perplexity calculations for column: new_label_withcontext\n",
      "Average Perplexity for withcontext: 63035.03316265359\n",
      "Calculating token-based perplexity for column: new_label_gpt_withoutemotion\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_withoutemotion\n",
      "Average Perplexity for gpt_withoutemotion: 28233.374812001217\n",
      "Calculating token-based perplexity for column: new_label_gpt_blocksize_256\n",
      "Completed token-based perplexity calculations for column: new_label_gpt_blocksize_256\n",
      "Average Perplexity for gpt_blocksize_256: 218114.86185134976\n"
     ]
    }
   ],
   "source": [
    "big_perplexity = perplexity_scores_average(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "BEST MODEL: \n",
      "gpt_withoutemotion: with a Perplexity Score of 28233.374812.\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# Find the best-performing model\n",
    "best_model = min(big_perplexity, key=big_perplexity.get)\n",
    "best_score = big_perplexity[best_model]\n",
    "\n",
    "# Print the results\n",
    "print(\"====================================================\")\n",
    "print(\"BEST MODEL: \")\n",
    "print(f\"{best_model}: with a Perplexity Score of {best_score:.6f}.\")\n",
    "print(\"====================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
