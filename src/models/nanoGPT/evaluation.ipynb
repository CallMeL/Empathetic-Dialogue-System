{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from chat import init_model as init_nanoGPT\n",
    "from chat import respond as get_respond_nanoGPT\n",
    "import torch\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data/emotion/validation/100_validation.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \n",
       "0                       Should have grabbed the gun.  \n",
       "1  Thank you!  It was so nice, I had no idea it w...  \n",
       "2        I do!  I'm feeling very optimistic about it  \n",
       "3  It was a medium amount of money but still, he ...  \n",
       "4  Well, I started walking much faster. It looked...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanxi.lin/Downloads/Project-ML/src/models/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/wholeConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withemotion/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withcontext/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=64/withoutemotion/singleConversation_withGPTdata/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/block_size=256/GPT_without_emotion/ckpt.pt\n",
      "number of parameters: 3.42M\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'withoutemotion_single': 'block_size=64/withoutemotion/singleConversation',\n",
    "    'withoutemotion_whole':'block_size=64/withoutemotion/wholeConversation',\n",
    "    'withemotion':'block_size=64/withemotion',\n",
    "    'withcontext': 'block_size=64/withcontext',\n",
    "    'gpt_withoutemotion': 'block_size=64/withoutemotion/singleConversation_withGPTdata',\n",
    "    'gpt_blocksize_256': 'block_size=256/GPT_without_emotion',\n",
    "}\n",
    "for model_type, model_path in model_list.items():\n",
    "    model_list[model_type] = init_nanoGPT(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_nanoGPT(row,model):\n",
    "    situation = row['Situation']\n",
    "    emotion = row['grouped_emotion']\n",
    "    human = row['empathetic_dialogues']\n",
    "    start = '<bot> ' + human + '<human>'\n",
    "    response, new_emotion, new_context = get_respond_nanoGPT(start, 1, model=model, enable_print=False)\n",
    "    return response #, new_emotion, new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    df[label] = df.apply(lambda row: get_response_from_nanoGPT(row, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./evaluation_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_withoutemotion_single</th>\n",
       "      <th>new_label_withoutemotion_whole</th>\n",
       "      <th>new_label_withemotion</th>\n",
       "      <th>new_label_withcontext</th>\n",
       "      <th>new_label_gpt_withoutemotion</th>\n",
       "      <th>new_label_gpt_blocksize_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "      <td>That's great! I hope you're able to do some g...</td>\n",
       "      <td>Yes, it was so mortifying on the news.</td>\n",
       "      <td>Did you get the chance to go?</td>\n",
       "      <td>That's cool.</td>\n",
       "      <td>Sounds like a fun movie! What did you find?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "      <td>I hope they are happy</td>\n",
       "      <td>I bet.  I've been there for a few years.  I'v...</td>\n",
       "      <td>It was. I was so happy that they had a great ...</td>\n",
       "      <td>Thank you so much! It was really nice to see ...</td>\n",
       "      <td>Thank you! How can I assist you today?</td>\n",
       "      <td>Thank you! I'm here to help with any question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "      <td>I did, I was able to stay with my friends.</td>\n",
       "      <td>I feel so bad for you.</td>\n",
       "      <td>I did, but I'm not sure I will do it again.</td>\n",
       "      <td>I really hope so. I am feeling a little happy...</td>\n",
       "      <td>I'm here to help! What do you need assistance...</td>\n",
       "      <td>I'm here to help! What do you need assistance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "      <td>I was so shocked by that. He had a great job ...</td>\n",
       "      <td>It was a brand new car. I was so upset.</td>\n",
       "      <td>I'm not sure about it.</td>\n",
       "      <td>I was so mad. He had a huge nail and we got t...</td>\n",
       "      <td>It sounds like you had a great time! What hap...</td>\n",
       "      <td>Yes, it wasn't a kind thing. I don't know wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "      <td>I was driving in the middle of the night and ...</td>\n",
       "      <td>I was so upset! I was so upset.</td>\n",
       "      <td>I was driving a new car and I was so mad!</td>\n",
       "      <td>I got to see a dead skunk in my yard.</td>\n",
       "      <td>I'm here to listen. What happened?</td>\n",
       "      <td>I'm here for you. What happened?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       Should have grabbed the gun.   \n",
       "1  Thank you!  It was so nice, I had no idea it w...   \n",
       "2        I do!  I'm feeling very optimistic about it   \n",
       "3  It was a medium amount of money but still, he ...   \n",
       "4  Well, I started walking much faster. It looked...   \n",
       "\n",
       "                     new_label_withoutemotion_single  \\\n",
       "0   That's great! I hope you're able to do some g...   \n",
       "1                             I hope they are happy    \n",
       "2        I did, I was able to stay with my friends.    \n",
       "3   I was so shocked by that. He had a great job ...   \n",
       "4   I was driving in the middle of the night and ...   \n",
       "\n",
       "                      new_label_withoutemotion_whole  \\\n",
       "0            Yes, it was so mortifying on the news.    \n",
       "1   I bet.  I've been there for a few years.  I'v...   \n",
       "2                            I feel so bad for you.    \n",
       "3           It was a brand new car. I was so upset.    \n",
       "4                   I was so upset! I was so upset.    \n",
       "\n",
       "                               new_label_withemotion  \\\n",
       "0                     Did you get the chance to go?    \n",
       "1   It was. I was so happy that they had a great ...   \n",
       "2       I did, but I'm not sure I will do it again.    \n",
       "3                            I'm not sure about it.    \n",
       "4         I was driving a new car and I was so mad!    \n",
       "\n",
       "                               new_label_withcontext  \\\n",
       "0                                      That's cool.    \n",
       "1   Thank you so much! It was really nice to see ...   \n",
       "2   I really hope so. I am feeling a little happy...   \n",
       "3   I was so mad. He had a huge nail and we got t...   \n",
       "4             I got to see a dead skunk in my yard.    \n",
       "\n",
       "                        new_label_gpt_withoutemotion  \\\n",
       "0       Sounds like a fun movie! What did you find?    \n",
       "1            Thank you! How can I assist you today?    \n",
       "2   I'm here to help! What do you need assistance...   \n",
       "3   It sounds like you had a great time! What hap...   \n",
       "4                I'm here to listen. What happened?    \n",
       "\n",
       "                         new_label_gpt_blocksize_256  \n",
       "0                                                     \n",
       "1   Thank you! I'm here to help with any question...  \n",
       "2   I'm here to help! What do you need assistance...  \n",
       "3   Yes, it wasn't a kind thing. I don't know wha...  \n",
       "4                  I'm here for you. What happened?   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(compared_column):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # To avoid 0 scores due to short sentences\n",
    "    for _, row in df.iterrows():\n",
    "        for ref, output in zip(row['labels'], row[compared_column]):\n",
    "            # Tokenize each sentence (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(reference_tokens, output_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = {}\n",
    "bleu_scores_average = {}\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # print(label)\n",
    "    bleu_scores[model_type] = get_bleu(label)\n",
    "    bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withoutemotion_single': 0.005745652374923808,\n",
       " 'withoutemotion_whole': 0.005187762489436854,\n",
       " 'withemotion': 0.004634877185739742,\n",
       " 'withcontext': 0.006159774135493186,\n",
       " 'gpt_withoutemotion': 0.0063584190169655086,\n",
       " 'gpt_blocksize_256': 0.006069653759755217}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(compared_column):\n",
    "    # Check for MPS device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "    # Calculate precision, recall, and F1 for each pair of reference and output\n",
    "    P, R, F1 = score(model_outputs, reference_sentences, lang='en', verbose=True, device = device)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2504f25eeb9a473da8ddcec7f23a788c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22eb92f51414136ac15026db903820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.66 seconds, 37.52 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c3c9990d7e4f8ba57c547e2c55a68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dbae399d4c4ba296a42b632b0f5b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.68 seconds, 59.35 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf458de96406480da1cdde97130add97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5626f71445449b9b2b0388d77fa763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.30 seconds, 77.03 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2f7db7122a4945a5b58e11a844e62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6c42228300405396b45f37dd09ba6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.28 seconds, 78.31 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cde4368864048679ed798b6afa44078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67287105719e4c6c930c75c478f96743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.78 seconds, 56.03 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509d42f174b748c39b790b42a057fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f574457aa9f4c31bd510377f2aaa5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.32 seconds, 75.99 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    }
   ],
   "source": [
    "bert_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    bert_scores[model_type] = {}\n",
    "    \n",
    "    # Calculate BERT score and assign it to the dictionary\n",
    "    bert_scores[model_type]['P'], bert_scores[model_type]['R'], bert_scores[model_type]['F1'] = calculate_bert_score(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Precision: 0.8582502603530884\n",
      "Average Recall: 0.8634032607078552\n",
      "Average F1: 0.8606156706809998\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Precision: 0.7520891427993774\n",
      "Average Recall: 0.7587206959724426\n",
      "Average F1: 0.7552400231361389\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Precision: 0.857521653175354\n",
      "Average Recall: 0.8639740943908691\n",
      "Average F1: 0.8604713678359985\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Precision: 0.8555678725242615\n",
      "Average Recall: 0.8625133633613586\n",
      "Average F1: 0.8588142991065979\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Precision: 0.8563526272773743\n",
      "Average Recall: 0.8623169660568237\n",
      "Average F1: 0.8591411113739014\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Precision: 0.562935471534729\n",
      "Average Recall: 0.5645362734794617\n",
      "Average F1: 0.5636320114135742\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # P avarage\n",
    "    P_average = sum(bert_scores[model_type]['P']) / len(bert_scores[model_type]['P'])\n",
    "    # R avarage\n",
    "    R_average = sum(bert_scores[model_type]['R']) / len(bert_scores[model_type]['R'])\n",
    "    # F1 avarage\n",
    "    F1_average = sum(bert_scores[model_type]['F1']) / len(bert_scores[model_type]['F1'])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Precision: {P_average}\")\n",
    "    print(f\"Average Recall: {R_average}\")\n",
    "    print(f\"Average F1: {F1_average}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # GLUE - Sentiment Analysis Evaluation (SST-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def evaluate_sentiment(compared_column):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load multi-class sentiment or emotion pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        device=0 if device == \"mps\" else -1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "\n",
    "    for i, (output, reference) in enumerate(zip(model_outputs, reference_sentences), start=1):\n",
    "        output_sentiment = sentiment_pipeline(output)[0]['label']\n",
    "        reference_sentiment = sentiment_pipeline(reference)[0]['label']\n",
    "        \n",
    "        score = 1 if output_sentiment == reference_sentiment else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    sentiment_scores[model_type] = {}\n",
    "    \n",
    "    sentiment_scores[model_type] = evaluate_sentiment(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Sentiment Score: 0.53\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Sentiment Score: 0.44\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Sentiment Score: 0.52\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Sentiment Score: 0.47\n",
      "--------------------------------------------------\n",
      "Model: gpt_withoutemotion\n",
      "Average Sentiment Score: 0.42\n",
      "--------------------------------------------------\n",
      "Model: gpt_blocksize_256\n",
      "Average Sentiment Score: 0.41\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    GLUE_average = sum(sentiment_scores[model_type]) / len(sentiment_scores[model_type])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Sentiment Score: {GLUE_average}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
