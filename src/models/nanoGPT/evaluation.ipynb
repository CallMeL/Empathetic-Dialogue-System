{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from chat import init_model as init_nanoGPT\n",
    "from chat import respond as get_respond_nanoGPT\n",
    "import torch\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../data/emotion/validation/100_validation.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \n",
       "0                       Should have grabbed the gun.  \n",
       "1  Thank you!  It was so nice, I had no idea it w...  \n",
       "2        I do!  I'm feeling very optimistic about it  \n",
       "3  It was a medium amount of money but still, he ...  \n",
       "4  Well, I started walking much faster. It looked...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained-saved/withoutemotion/singleConversation/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/withoutemotion/wholeConversation/ckpt.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanxi.lin/Downloads/Project-ML/src/models/nanoGPT/chat.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/withemotion/ckpt.pt\n",
      "number of parameters: 3.42M\n",
      "Loading model from: ../trained-saved/withcontext/ckpt.pt\n",
      "number of parameters: 3.42M\n"
     ]
    }
   ],
   "source": [
    "model_list = {\n",
    "    'withoutemotion_single': 'withoutemotion/singleConversation',\n",
    "    'withoutemotion_whole':'withoutemotion/wholeConversation',\n",
    "    'withemotion':'withemotion',\n",
    "    'withcontext': 'withcontext'\n",
    "}\n",
    "for model_type, model_path in model_list.items():\n",
    "    model_list[model_type] = init_nanoGPT(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_nanoGPT(row,model):\n",
    "    situation = row['Situation']\n",
    "    emotion = row['grouped_emotion']\n",
    "    human = row['empathetic_dialogues']\n",
    "    start = '<bot> ' + human + '<human>'\n",
    "    response, new_emotion, new_context = get_respond_nanoGPT(start, 1, model=model, enable_print=False)\n",
    "    return response #, new_emotion, new_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    df[label] = df.apply(lambda row: get_response_from_nanoGPT(row, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./evaluation_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Situation</th>\n",
       "      <th>grouped_emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>new_label_withoutemotion_single</th>\n",
       "      <th>new_label_withoutemotion_whole</th>\n",
       "      <th>new_label_withemotion</th>\n",
       "      <th>new_label_withcontext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last night I heard strange noises coming from ...</td>\n",
       "      <td>afraid</td>\n",
       "      <td>In the middle of the night I heard some weird ...</td>\n",
       "      <td>Should have grabbed the gun.</td>\n",
       "      <td>Oh no! Was it a nasty thing?</td>\n",
       "      <td></td>\n",
       "      <td>I've been there and I think it's a bit scary.</td>\n",
       "      <td>I would have been a bad one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mom and sister threw me a baby shower when ...</td>\n",
       "      <td>excited</td>\n",
       "      <td>that was very nice of them congratulations</td>\n",
       "      <td>Thank you!  It was so nice, I had no idea it w...</td>\n",
       "      <td>It was very nice</td>\n",
       "      <td>Thank you. I'm happy with that.</td>\n",
       "      <td>Yeah they really did. I was so excited and ha...</td>\n",
       "      <td>Yes, it was a big surprise. I was so happy fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just applied for a new job.  After the inter...</td>\n",
       "      <td>grateful</td>\n",
       "      <td>Oh really, do you feel like you did a great job?</td>\n",
       "      <td>I do!  I'm feeling very optimistic about it</td>\n",
       "      <td>I do, I hope I get it.</td>\n",
       "      <td>I am very prepared. I will be ready for the r...</td>\n",
       "      <td>I did. It was so nice. It was so nice that I ...</td>\n",
       "      <td>I did, and I was really happy to find it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I loaned some money to my friend at work. Turn...</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>Wow! What a jerk for him to up and leave with ...</td>\n",
       "      <td>It was a medium amount of money but still, he ...</td>\n",
       "      <td>It was a good idea!</td>\n",
       "      <td>Yes! He was very lucky.</td>\n",
       "      <td>It was a promotion, but I was not sure it was...</td>\n",
       "      <td>I was so mad. He was a jerk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was out walking late last night and seen som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh my god. What happened?</td>\n",
       "      <td>Well, I started walking much faster. It looked...</td>\n",
       "      <td>I was sitting there, but I'm not sure if it w...</td>\n",
       "      <td>It was so gross.  It was so gross!</td>\n",
       "      <td>I was in a car accident. I was so upset.</td>\n",
       "      <td>Oh my gosh, I hope they don't have to help me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Situation grouped_emotion  \\\n",
       "0  Last night I heard strange noises coming from ...          afraid   \n",
       "1  My mom and sister threw me a baby shower when ...         excited   \n",
       "2  I just applied for a new job.  After the inter...        grateful   \n",
       "3  I loaned some money to my friend at work. Turn...         annoyed   \n",
       "4  I was out walking late last night and seen som...             NaN   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  In the middle of the night I heard some weird ...   \n",
       "1         that was very nice of them congratulations   \n",
       "2   Oh really, do you feel like you did a great job?   \n",
       "3  Wow! What a jerk for him to up and leave with ...   \n",
       "4                          Oh my god. What happened?   \n",
       "\n",
       "                                              labels  \\\n",
       "0                       Should have grabbed the gun.   \n",
       "1  Thank you!  It was so nice, I had no idea it w...   \n",
       "2        I do!  I'm feeling very optimistic about it   \n",
       "3  It was a medium amount of money but still, he ...   \n",
       "4  Well, I started walking much faster. It looked...   \n",
       "\n",
       "                     new_label_withoutemotion_single  \\\n",
       "0                      Oh no! Was it a nasty thing?    \n",
       "1                                  It was very nice    \n",
       "2                            I do, I hope I get it.    \n",
       "3                               It was a good idea!    \n",
       "4   I was sitting there, but I'm not sure if it w...   \n",
       "\n",
       "                      new_label_withoutemotion_whole  \\\n",
       "0                                                      \n",
       "1                   Thank you. I'm happy with that.    \n",
       "2   I am very prepared. I will be ready for the r...   \n",
       "3                           Yes! He was very lucky.    \n",
       "4                It was so gross.  It was so gross!    \n",
       "\n",
       "                               new_label_withemotion  \\\n",
       "0     I've been there and I think it's a bit scary.    \n",
       "1   Yeah they really did. I was so excited and ha...   \n",
       "2   I did. It was so nice. It was so nice that I ...   \n",
       "3   It was a promotion, but I was not sure it was...   \n",
       "4          I was in a car accident. I was so upset.    \n",
       "\n",
       "                               new_label_withcontext  \n",
       "0                      I would have been a bad one.   \n",
       "1   Yes, it was a big surprise. I was so happy fo...  \n",
       "2        I did, and I was really happy to find it.    \n",
       "3                      I was so mad. He was a jerk.   \n",
       "4    Oh my gosh, I hope they don't have to help me.   "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(compared_column):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction().method1  # To avoid 0 scores due to short sentences\n",
    "    for _, row in df.iterrows():\n",
    "        for ref, output in zip(row['labels'], row[compared_column]):\n",
    "            # Tokenize each sentence (split by words)\n",
    "            reference_tokens = [ref.split()]  # BLEU expects a list of lists for references\n",
    "            output_tokens = output.split()\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = sentence_bleu(reference_tokens, output_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu)\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_label_withoutemotion_single\n",
      "new_label_withoutemotion_whole\n",
      "new_label_withemotion\n",
      "new_label_withcontext\n"
     ]
    }
   ],
   "source": [
    "bleu_scores = {}\n",
    "bleu_scores_average = {}\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # print(label)\n",
    "    bleu_scores[model_type] = get_bleu(label)\n",
    "    bleu_scores_average[model_type] = sum(bleu_scores[model_type]) / len(bleu_scores[model_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withoutemotion_single': 0.006172121992169218,\n",
       " 'withoutemotion_whole': 0.005432207996084309,\n",
       " 'withemotion': 0.005625406761168866,\n",
       " 'withcontext': 0.0061107630233090135}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(compared_column):\n",
    "    # Check for MPS device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "    # Calculate precision, recall, and F1 for each pair of reference and output\n",
    "    P, R, F1 = score(model_outputs, reference_sentences, lang='en', verbose=True, device = device)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09cf87f85c449e19093073289cc8bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395db8d526f4ff89910125b1d3d7289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.25 seconds, 80.29 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39b8c748418426eab11a901152040d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25de1f682aff4d379566abc9224db541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.16 seconds, 86.46 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9bfc6ee46c4f22b81eed2a3df944ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8355271141164488917fafaa483172a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.24 seconds, 80.47 sentences/sec\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d97b812b224bd8b44558e29fd325ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2dafdf68694d07a2a49047c3e8f005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.18 seconds, 84.41 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "bert_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    bert_scores[model_type] = {}\n",
    "    \n",
    "    # Calculate BERT score and assign it to the dictionary\n",
    "    bert_scores[model_type]['P'], bert_scores[model_type]['R'], bert_scores[model_type]['F1'] = calculate_bert_score(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Precision: 0.8552975654602051\n",
      "Average Recall: 0.8628495931625366\n",
      "Average F1: 0.8588349223136902\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Precision: 0.7506046891212463\n",
      "Average Recall: 0.7574512958526611\n",
      "Average F1: 0.7538419961929321\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Precision: 0.8577821254730225\n",
      "Average Recall: 0.8660477995872498\n",
      "Average F1: 0.861606776714325\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Precision: 0.8571752309799194\n",
      "Average Recall: 0.8627082705497742\n",
      "Average F1: 0.8597039580345154\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    # P avarage\n",
    "    P_average = sum(bert_scores[model_type]['P']) / len(bert_scores[model_type]['P'])\n",
    "    # R avarage\n",
    "    R_average = sum(bert_scores[model_type]['R']) / len(bert_scores[model_type]['R'])\n",
    "    # F1 avarage\n",
    "    F1_average = sum(bert_scores[model_type]['F1']) / len(bert_scores[model_type]['F1'])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Precision: {P_average}\")\n",
    "    print(f\"Average Recall: {R_average}\")\n",
    "    print(f\"Average F1: {F1_average}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # GLUE - Sentiment Analysis Evaluation (SST-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def evaluate_sentiment(compared_column):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load multi-class sentiment or emotion pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        device=0 if device == \"mps\" else -1\n",
    "    )\n",
    "    \n",
    "    scores = []\n",
    "    model_outputs = df['labels']\n",
    "    reference_sentences = df[compared_column]\n",
    "\n",
    "    if len(model_outputs) != len(reference_sentences):\n",
    "        raise ValueError(\"Mismatch in lengths: model_outputs and reference_sentences must be of the same length.\")\n",
    "    # Convert model outputs and reference sentences to strings\n",
    "    model_outputs = [str(output) for output in model_outputs]\n",
    "    reference_sentences = [str(ref) for ref in reference_sentences]\n",
    "\n",
    "    for i, (output, reference) in enumerate(zip(model_outputs, reference_sentences), start=1):\n",
    "        output_sentiment = sentiment_pipeline(output)[0]['label']\n",
    "        reference_sentiment = sentiment_pipeline(reference)[0]['label']\n",
    "        \n",
    "        score = 1 if output_sentiment == reference_sentiment else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = {}\n",
    "\n",
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    sentiment_scores[model_type] = {}\n",
    "    \n",
    "    sentiment_scores[model_type] = evaluate_sentiment(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model: withoutemotion_single\n",
      "Average Sentiment Score: 0.52\n",
      "--------------------------------------------------\n",
      "Model: withoutemotion_whole\n",
      "Average Sentiment Score: 0.49\n",
      "--------------------------------------------------\n",
      "Model: withemotion\n",
      "Average Sentiment Score: 0.55\n",
      "--------------------------------------------------\n",
      "Model: withcontext\n",
      "Average Sentiment Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "for model_type, model in model_list.items():\n",
    "    label = 'new_label_' + model_type\n",
    "    GLUE_average = sum(sentiment_scores[model_type]) / len(sentiment_scores[model_type])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Average Sentiment Score: {GLUE_average}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
